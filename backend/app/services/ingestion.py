import trafilatura
from trafilatura.sitemaps import sitemap_search
import json
from datetime import datetime
from openai import OpenAI
from ..models import Article, Entity, ArticleEntity
from ..database import SessionLocal

# LLM ve VeritabanÄ± AyarlarÄ±
client = OpenAI() # .env dosyasÄ±ndan key'i otomatik okur

def process_universal_target(url, user_id):
    """
    Bu fonksiyon 'Evrensel GiriÅŸ KapÄ±sÄ±'dÄ±r.
    URL ana sayfa mÄ± yoksa tekil makale mi karar verir ve akÄ±ÅŸÄ± baÅŸlatÄ±r.
    """
    db = SessionLocal()
    print(f"ğŸš€ Evrensel tarama baÅŸladÄ±: {url}")

    # ADIM 1: KEÅÄ°F (DISCOVERY)
    # Ã–nce bunun bir sitemap veya ana sayfa olup olmadÄ±ÄŸÄ±na bakalÄ±m
    urls_to_process = []
    
    # Sitemap aramasÄ± yap (Trafilatura'nÄ±n evrensel sitemap bulucusu)
    try:
        sitemap_links = sitemap_search(url)
    except Exception as e:
        print(f"Sitemap search error: {e}")
        sitemap_links = []
    
    if sitemap_links:
        print(f"ğŸ“¦ Sitemap bulundu! {len(sitemap_links)} link iÃ§eriyor.")
        urls_to_process = sitemap_links
    else:
        # Sitemap yoksa, sayfayÄ± tekil bir makale gibi varsayalÄ±m
        print("â„¹ï¸ Sitemap bulunamadÄ±, tekil URL iÅŸleniyor.")
        urls_to_process = [url]

    # Bulunan her URL iÃ§in Evrensel Ã‡Ä±karma iÅŸlemini yap
    processed_count = 0
    for target_url in urls_to_process:
        # Zaten ekli mi kontrol et (Tekrar iÅŸleme)
        exists = db.query(Article).filter(Article.url == target_url).first()
        if exists:
            continue
            
        success = pipeline_execution(target_url, db, user_id)
        if success:
            processed_count += 1
            
    db.close()
    return f"{processed_count} makale baÅŸarÄ±yla evrensel analizden geÃ§irildi."

def pipeline_execution(url, db, user_id):
    """
    Tek bir URL'yi 3 aÅŸamalÄ± evrensel filtreden geÃ§irir.
    """
    try:
        # ADIM 2: EVRENSEL Ã‡IKARMA (EXTRACTION)
        downloaded = trafilatura.fetch_url(url)
        
        if downloaded is None:
            return False
            
        # HTML'den temiz metni ayÄ±kla (ReklamlarÄ±, menÃ¼leri atar)
        text_content = trafilatura.extract(downloaded, include_comments=False)
        
        if not text_content or len(text_content) < 500:
            # Ã‡ok kÄ±sa iÃ§erikleri (Ä°letiÅŸim sayfasÄ± vb.) atla
            return False

        # ADIM 3: ANLAMLANDIRMA & TARÄ°H (INTELLIGENCE via LLM)
        # Trafilatura tarihi bazen kaÃ§Ä±rabilir, en garantisi LLM'e sormaktÄ±r.
        # AyrÄ±ca "Bu bir blog yazÄ±sÄ± mÄ±?" kontrolÃ¼nÃ¼ de LLM yapar.
        
        analysis = analyze_with_llm(text_content[:3000]) # Ä°lk 3000 karakter yeterli
        
        if not analysis.get("is_blog_post"):
            print(f"â© Blog yazÄ±sÄ± deÄŸil, atlanÄ±yor: {url}")
            return False

        # ADIM 4: VEKTÃ–RLEÅTÄ°RME (EMBEDDING)
        # BaÅŸlÄ±k + Ã–zet bilgisini vektÃ¶re Ã§evir
        vector_text = f"{analysis['title']} {analysis['summary']}"
        embedding = get_embedding(vector_text)

        if not embedding or len(embedding) == 0:
            print(f"âŒ Embedding oluÅŸturulamadÄ± (Muhtemelen OpenAI kotasÄ± doldu): {url}")
            return False

        # ADIM 5: KAYIT (STORAGE)
        publish_date_val = analysis.get('publish_date')
        if hasattr(publish_date_val, 'startswith'): # is boolean check if string
             try:
                 publish_date_val = datetime.strptime(publish_date_val, '%Y-%m-%d')
             except:
                 publish_date_val = datetime.utcnow()
        else:
             publish_date_val = datetime.utcnow()

        new_article = Article(
            url=url,
            title=analysis['title'],
            content_summary=analysis['summary'],
            publish_date=publish_date_val,
            embedding=embedding,
            user_id=user_id,
            target_keyword=analysis.get('target_keyword', analysis['title']),
            html_structure_sample=str(downloaded)[:100000] if downloaded else None, # Ä°lk 100KB'Ä± sakla
            raw_content_hash=str(hash(text_content)) # DeÄŸiÅŸiklik takibi iÃ§in
        )
        
        db.add(new_article)
        db.commit()
        
        # Entity KaydÄ±
        if analysis.get("entities"):
            for ent in analysis["entities"]:
                # Entity var mÄ± diye bak, yoksa oluÅŸtur
                existing_entity = db.query(Entity).filter(Entity.name == ent['name']).first()
                if not existing_entity:
                    existing_entity = Entity(name=ent['name'], category=ent['category'])
                    db.add(existing_entity)
                    db.commit()
                
                # Ä°liÅŸkiyi kaydet
                rel = ArticleEntity(article_id=new_article.id, entity_id=existing_entity.id)
                db.add(rel)
            db.commit()

        print(f"âœ… Eklendi: {analysis['title']}")
        return True

    except Exception as e:
        print(f"âŒ Hata ({url}): {e}")
        return False

def analyze_with_llm(text_chunk):
    """
    GPT-4o-mini kullanarak metinden yapÄ±sal veri Ã§Ä±karÄ±r.
    """
    prompt = """
    AÅŸaÄŸÄ±daki metni analiz et ve JSON formatÄ±nda ÅŸu bilgileri ver:
    1. is_blog_post: (boolean) Bu bir makale mi?
    2. title: (string) BaÅŸlÄ±k.
    3. summary: (string) 2 cÃ¼mlelik Ã¶zet.
    4. publish_date: (string) YYYY-MM-DD. Yoksa null.
    
    5. target_keyword: (string) Bu makaleyi en iyi tanÄ±mlayan 1-2 kelimelik SEO anahtar kelimesi (Ã–rn: 'SEO FiyatlarÄ±', 'Python Dersleri'). BaÅŸlÄ±ktan tÃ¼ret.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Sen bir veri Ã§Ä±karma uzmanÄ±sÄ±n. Sadece JSON dÃ¶ndÃ¼r."},
                {"role": "user", "content": f"{prompt}\n\nMETÄ°N:\n{text_chunk}"}
            ],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"LLM Error: {e}")
        return {"is_blog_post": False}

def get_embedding(text):
    """
    OpenAI Text Embedding 3 Small ile vektÃ¶r oluÅŸturur.
    """
    try:
        response = client.embeddings.create(
            input=text,
            model="text-embedding-3-small"
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Embedding Error: {e}")
        return []
